{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, ifft, fftfreq\n",
    "from scipy.signal import argrelextrema\n",
    "import time\n",
    "import peakutils\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to save an object with `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open('../training_files/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Ising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/l04_full_state_phase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size= 0.2, random_state= 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(list(train_data['state']), dtype= torch.float)\n",
    "y_train = torch.tensor(np.array(train_data[['ordered', 'desordered']]) ,dtype= torch.float)\n",
    "\n",
    "X_test = torch.tensor(list(test_data['state']), dtype= torch.float)\n",
    "y_test = torch.tensor(np.array(test_data[['ordered', 'desordered']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  Shwartz-Ziv/Tishby data\n",
    "\n",
    "We make use of the the functions defined in `utils` by Saxe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_file.py\n",
    "import sys\n",
    "sys.path.insert(0, '../estimators')\n",
    "\n",
    "import utils\n",
    "\n",
    "train, test = utils.get_IB_data('2017_12_21_16_51_3_275766')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train.X, dtype= torch.float)\n",
    "y_train = torch.tensor(train.Y, dtype= torch.float)\n",
    "\n",
    "X_test = torch.tensor(test.X, dtype= torch.float)\n",
    "y_test = torch.tensor(test.Y, dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3277, 12])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([819, 12])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the network: `Net` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''\n",
    "    net_layer: list with the number of neurons for each network layer, [n_imput, ..., n_output]\n",
    "    '''\n",
    "    def __init__(self, layers_size, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for k in range(len(layers_size) - 1):\n",
    "            self.layers.append(nn.Linear(layers_size[k], layers_size[k+1]))\n",
    "            \n",
    "        # Output layer # Here we could choose a different activation function\n",
    "        self.out = nn.Linear(layers_size[-1], out_size)\n",
    "        \n",
    "        ###### WEIGHT INITIALIZATION\n",
    "        for m in self.layers:\n",
    "            nn.init.normal_(m.weight, mean= 0, std= 1/np.sqrt(100*len(layers_size)))\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "        nn.init.normal_(self.out.weight, mean= 0, std= 1/np.sqrt(100*len(layers_size)))\n",
    "        nn.init.constant_(self.out.bias, 0.0)\n",
    "        ############################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        j = 1\n",
    "        #act_state_batch = []\n",
    "        act_st_batch = {\n",
    "            'activity' : []\n",
    "            }\n",
    "        \n",
    "        for layer in self.layers:\n",
    "                       \n",
    "            x = F.tanh(layer(x))\n",
    "                      \n",
    "            act_st_batch['activity'].append( x.detach().numpy())\n",
    "            \n",
    "            j = j + 1\n",
    "            \n",
    "        output= F.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        act_st_batch['activity'].append( output.detach().numpy())\n",
    "            \n",
    "        \n",
    "        return output, act_st_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "\n",
    "### Initializing the class `Net` and defining an optimizer and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=12, out_features=3, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n",
      "{'activity': [array([[-0.14506368,  0.07718648, -0.22604144],\n",
      "       [ 0.18322146,  0.1474182 , -0.14231911],\n",
      "       [ 0.02592443,  0.09238657, -0.00644962],\n",
      "       [ 0.11892071,  0.21342322, -0.10445723],\n",
      "       [ 0.10040651,  0.15205173, -0.20389418]], dtype=float32), array([[0.49683657, 0.5031634 ],\n",
      "       [0.49393514, 0.5060649 ],\n",
      "       [0.49755132, 0.5024486 ],\n",
      "       [0.4931092 , 0.5068908 ],\n",
      "       [0.4937353 , 0.50626475]], dtype=float32)]}\n",
      "2\n",
      "5\n",
      "5\n",
      "1\n",
      "False\n",
      "{'activity': [array([[ 0.09266253,  0.07034681, -0.10214622],\n",
      "       [ 0.00514269, -0.09910725, -0.29751828],\n",
      "       [ 0.19704197,  0.29849377, -0.04346497],\n",
      "       [ 0.13904627,  0.13046984, -0.18014275],\n",
      "       [ 0.03931467,  0.21123177, -0.01356388]], dtype=float32), array([[0.498619  , 0.50138104],\n",
      "       [0.5022468 , 0.49775323],\n",
      "       [0.49226692, 0.5077331 ],\n",
      "       [0.49606907, 0.50393087],\n",
      "       [0.4960621 , 0.50393796]], dtype=float32)]}\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "True\n",
      "{'activity': [array([[-0.14506368,  0.07718648, -0.22604144],\n",
      "       [ 0.18322146,  0.1474182 , -0.14231911],\n",
      "       [ 0.02592443,  0.09238657, -0.00644962],\n",
      "       [ 0.11892071,  0.21342322, -0.10445723],\n",
      "       [ 0.10040651,  0.15205173, -0.20389418],\n",
      "       [ 0.09266253,  0.07034681, -0.10214622],\n",
      "       [ 0.00514269, -0.09910725, -0.29751828],\n",
      "       [ 0.19704197,  0.29849377, -0.04346497],\n",
      "       [ 0.13904627,  0.13046984, -0.18014275],\n",
      "       [ 0.03931467,  0.21123177, -0.01356388]], dtype=float32), array([[0.49683657, 0.5031634 ],\n",
      "       [0.49393514, 0.5060649 ],\n",
      "       [0.49755132, 0.5024486 ],\n",
      "       [0.4931092 , 0.5068908 ],\n",
      "       [0.4937353 , 0.50626475],\n",
      "       [0.498619  , 0.50138104],\n",
      "       [0.5022468 , 0.49775323],\n",
      "       [0.49226692, 0.5077331 ],\n",
      "       [0.49606907, 0.50393087],\n",
      "       [0.4960621 , 0.50393796]], dtype=float32)]}\n",
      "{'activity': [array([[-0.04290669,  0.17161897, -0.11846994],\n",
      "       [ 0.16765672, -0.0483562 , -0.08755704],\n",
      "       [ 0.12499005,  0.08637048, -0.15408626],\n",
      "       [ 0.21127334, -0.04807955, -0.02820483],\n",
      "       [ 0.010777  , -0.2330356 ,  0.01957875]], dtype=float32), array([[0.49567562, 0.50432444],\n",
      "       [0.49991888, 0.5000811 ],\n",
      "       [0.49602762, 0.50397235],\n",
      "       [0.49998584, 0.5000142 ],\n",
      "       [0.5075804 , 0.49241963]], dtype=float32)]}\n",
      "2\n",
      "5\n",
      "5\n",
      "3\n",
      "True\n",
      "{'activity': [array([[-0.14506368,  0.07718648, -0.22604144],\n",
      "       [ 0.18322146,  0.1474182 , -0.14231911],\n",
      "       [ 0.02592443,  0.09238657, -0.00644962],\n",
      "       [ 0.11892071,  0.21342322, -0.10445723],\n",
      "       [ 0.10040651,  0.15205173, -0.20389418],\n",
      "       [ 0.09266253,  0.07034681, -0.10214622],\n",
      "       [ 0.00514269, -0.09910725, -0.29751828],\n",
      "       [ 0.19704197,  0.29849377, -0.04346497],\n",
      "       [ 0.13904627,  0.13046984, -0.18014275],\n",
      "       [ 0.03931467,  0.21123177, -0.01356388],\n",
      "       [-0.04290669,  0.17161897, -0.11846994],\n",
      "       [ 0.16765672, -0.0483562 , -0.08755704],\n",
      "       [ 0.12499005,  0.08637048, -0.15408626],\n",
      "       [ 0.21127334, -0.04807955, -0.02820483],\n",
      "       [ 0.010777  , -0.2330356 ,  0.01957875]], dtype=float32), array([[0.49683657, 0.5031634 ],\n",
      "       [0.49393514, 0.5060649 ],\n",
      "       [0.49755132, 0.5024486 ],\n",
      "       [0.4931092 , 0.5068908 ],\n",
      "       [0.4937353 , 0.50626475],\n",
      "       [0.498619  , 0.50138104],\n",
      "       [0.5022468 , 0.49775323],\n",
      "       [0.49226692, 0.5077331 ],\n",
      "       [0.49606907, 0.50393087],\n",
      "       [0.4960621 , 0.50393796],\n",
      "       [0.49567562, 0.50432444],\n",
      "       [0.49991888, 0.5000811 ],\n",
      "       [0.49602762, 0.50397235],\n",
      "       [0.49998584, 0.5000142 ],\n",
      "       [0.5075804 , 0.49241963]], dtype=float32)]}\n"
     ]
    }
   ],
   "source": [
    "########## DEFINING NETWORK ARCHITETURE\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "out_size = y_train.shape[1]\n",
    "#layers_size = [input_size, 10, 7, 5, 4, 3]\n",
    "\n",
    "layers_size = [input_size, 3]\n",
    "\n",
    "net = Net(layers_size, out_size)     \n",
    "print(net)\n",
    "\n",
    "######### DEFINING OPTIMIZER AND LOSS FUNCTION\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.004)\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "######## INITIALIZING TRAINING AND STORING IMPORT10,7,5,4,3ANT INFORMATION\n",
    "\n",
    "log_dic = {\n",
    "    'epoch': [],\n",
    "    'loss' : [],\n",
    "    'loss_gen' : [],\n",
    "    'data' : []\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    weights = {'weights_norm' : [],\n",
    "               'grad_mean'    : [],\n",
    "               'grad_std'     : []\n",
    "              }  # Recording weights norm\n",
    "    \n",
    "       \n",
    "    loss_epoch = []\n",
    "    \n",
    "    #activations_epoch = {'activations': []}\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    k_aux = 1\n",
    "    \n",
    "    ########################### LOOP OVER THE MINI-BATCHES ############################\n",
    "\n",
    "    \n",
    "    \n",
    "    for _, (input_data, target) in enumerate(train_loader):\n",
    "        \n",
    "        prediction, act_state_batch = net(input_data)     # input x and predict based on x\n",
    "                                                          # act_state provides the activation values for each neuron\n",
    "                                                          # in each layer for each batch\n",
    "        \n",
    "        loss = loss_func(prediction, target)     # must be (1. nn output, 2. target)\n",
    "        \n",
    "        loss_epoch.append(loss.item())\n",
    "          \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(act_state_batch)\n",
    "        print(len(act_state_batch['activity']))\n",
    "        print(len(act_state_batch['activity'][0]))\n",
    "        print(len(act_state_batch['activity'][1]))\n",
    "        #print(len(act_state_batch['activity'][2]))\n",
    "        #print(len(act_state_batch['activity'][3]))\n",
    "        #print(len(act_state_batch['activity'][4]))\n",
    "        #print(len(act_state_batch['activity'][5]))\n",
    "        print(len(loss_epoch))\n",
    "        print(len(loss_epoch) > 1)\n",
    "        #break\n",
    "        \n",
    "        \n",
    "        ###### STORING ACTIVATIONS ######\n",
    "        \n",
    "        if len(loss_epoch) > 1:\n",
    "            for r in range(len(act_state_batch['activity'])):\n",
    "                act_state_batch['activity'][r] = np.append(act_state_batch_last['activity'][r], \n",
    "                                                           act_state_batch['activity'][r], axis= 0)\n",
    "                \n",
    "            print(act_state_batch)\n",
    "                \n",
    "        \n",
    "            \n",
    "        act_state_batch_last = act_state_batch.copy()\n",
    "        \n",
    "        \n",
    "        if k_aux > 2:\n",
    "            break\n",
    "        \n",
    "        k_aux = k_aux + 1\n",
    "        \n",
    "        #################################\n",
    "        \n",
    "    #for l in range(len(layers_size)):\n",
    "    #    print(act_state_batch['activity'][l].shape)\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###################################################################################\n",
    "    \n",
    "\n",
    "       \n",
    "    #activations_epoch['activations'].append(act_state_batch)       \n",
    "         \n",
    "    #print(act_state_batch)\n",
    "    #print(a)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    ##################################################################################\n",
    "    \n",
    "    for n in range(0, 2*len(layers_size), 2):\n",
    "        \n",
    "        W = list(net.parameters())[n]\n",
    "        \n",
    "        weights['weights_norm'].append( np.linalg.norm(W.detach().numpy(), ord=2) )\n",
    "        weights['grad_mean'].append( np.absolute(W.grad.mean().item()) )\n",
    "        weights['grad_std'].append( W.grad.std().item())        \n",
    "    \n",
    "    ############## RECORDING\n",
    "    log_dic['epoch'].append(epoch)\n",
    "    log_dic['loss'].append(np.mean(loss_epoch))\n",
    "    test_pred, _ = net(X_test)\n",
    "    log_dic['loss_gen'].append(loss_func(test_pred, y_test ).item())\n",
    "    #log_dic['data'].append({**act_state_batch})\n",
    "    #if do_report(epoch):\n",
    "    log_dic['data'].append({**act_state_batch, **weights})\n",
    "    \n",
    "    #if epoch == 2:\n",
    "    #    break\n",
    "    \n",
    "    ##################################################################################\n",
    "    \n",
    "    \n",
    "    #optimizer.step()        # apply gradients\n",
    "    \n",
    "     ############ TIME COMPUTING ALL THE INFORMATION ########\n",
    "    t1 = time.time()\n",
    "    ########################################################\n",
    "    \n",
    "    ########### TRAINING STATUS ########\n",
    "    print('Epoch %d, Loss= %.10f, Time= %.4f' % (epoch, np.mean(loss_epoch), t1-t0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48523843, 0.5147615 ],\n",
       "       [0.4854554 , 0.5145446 ],\n",
       "       [0.4856184 , 0.5143816 ],\n",
       "       ...,\n",
       "       [0.5000005 , 0.49999955],\n",
       "       [0.50000024, 0.49999976],\n",
       "       [0.5       , 0.49999997]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dic['data'][0]['activity'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_dic['epoch'], log_dic['loss'], label= 'Training error')\n",
    "plt.plot(log_dic['epoch'], log_dic['loss_gen'], label= 'Test error')\n",
    "plt.xlabel('Epoch', fontsize= 15)\n",
    "plt.ylabel('Loss', fontsize= 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_dic['epoch'][::10], log_dic['loss'][::10], label= 'Training error')\n",
    "plt.plot(log_dic['epoch'][::10], log_dic['loss_gen'][::10], label= 'Test error')\n",
    "plt.xlabel('Epoch', fontsize= 15)\n",
    "plt.ylabel('Loss', fontsize= 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look in what is stored in `log_dic`\n",
    "\n",
    "Epoch number zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dic['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `log_dic['data'][0]['activity']` we access activation values in the first epoch for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dic['data'][0]['activity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is `log_dic['data'][0]['activity'][0]`, the second `log_dic['data'][0]['activity'][1]` and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dic['data'][0]['activity'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation values for the last layers can be obtained without the knowledge about the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dic['data'][0]['activity'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Python dictionary to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(log_dic, 'tishby_mini_batch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
